# AI Threat Modeler Configuration
# Copy this file to ~/.threat-modeler.conf or set environment variables directly

# ============================================
# AI Provider Selection
# ============================================
# Options: "claude" or "ollama"
# Default: ollama (if compiled with ollama feature), claude otherwise
AI_PROVIDER=ollama

# ============================================
# Ollama Configuration (Local AI)
# ============================================
# Ollama server URL
OLLAMA_HOST=http://localhost:11434

# Model to use for threat analysis
# Recommended models:
#   - llama3.1:70b (best quality, requires powerful GPU)
#   - llama3.1:8b (fast, good quality, works on most systems)
#   - mixtral:8x7b (good balance)
#   - codellama:34b (excellent for infrastructure code)
OLLAMA_MODEL=llama3.1:70b

# ============================================
# Claude API Configuration (Cloud AI)
# ============================================
# Get your API key from https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-...

# ============================================
# General Settings
# ============================================
# Logging level (error, warn, info, debug, trace)
RUST_LOG=info

# ============================================
# Performance Tuning (Advanced)
# ============================================
# Number of concurrent analyses (for batch scanning)
# CONCURRENT_ANALYSES=4

# Cache analysis results (future feature)
# ENABLE_CACHE=true
# CACHE_DIR=~/.threat-modeler-cache
